{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd945b2c",
   "metadata": {},
   "source": [
    "# ðŸ·ï¸ Project: Tag, You're Nash!\n",
    "## Analyzing the Impact of Reward Structures on Strategic Stability in MARL\n",
    "\n",
    "**Submitted by:** Omer Toledano & Eliya Naomi Aharon\n",
    "\n",
    "**Course:** Multi-Agent Systems (MAS) | Ben-Gurion University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad64a303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting supersuit\n",
      "  Downloading supersuit-3.10.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting stable-baselines3\n",
      "  Downloading stable_baselines3-2.7.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: pandas in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (3.10.3)\n",
      "Collecting pettingzoo[mpe]\n",
      "  Downloading pettingzoo-1.25.0-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from pettingzoo[mpe]) (1.26.4)\n",
      "Collecting gymnasium>=1.0.0 (from pettingzoo[mpe])\n",
      "  Downloading gymnasium-1.2.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pygame>=2.3.0 (from pettingzoo[mpe])\n",
      "  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting tinyscaler>=1.2.6 (from supersuit)\n",
      "  Downloading tinyscaler-1.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from stable-baselines3) (2.8.0)\n",
      "Collecting cloudpickle (from stable-baselines3)\n",
      "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from gymnasium>=1.0.0->pettingzoo[mpe]) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium>=1.0.0->pettingzoo[mpe])\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Requirement already satisfied: six>=1.5 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: filelock in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.16.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from triton==3.4.0->torch<3.0,>=2.3->stable-baselines3) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/omertole/.conda/envs/my_env/lib/python3.10/site-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
      "Downloading supersuit-3.10.0-py3-none-any.whl (50 kB)\n",
      "Downloading stable_baselines3-2.7.1-py3-none-any.whl (188 kB)\n",
      "Downloading gymnasium-1.2.3-py3-none-any.whl (952 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m952.1/952.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tinyscaler-1.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (522 kB)\n",
      "Downloading pettingzoo-1.25.0-py3-none-any.whl (852 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m852.5/852.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, tinyscaler, pygame, cloudpickle, gymnasium, supersuit, pettingzoo, stable-baselines3\n",
      "Successfully installed cloudpickle-3.1.2 farama-notifications-0.0.4 gymnasium-1.2.3 pettingzoo-1.25.0 pygame-2.6.1 stable-baselines3-2.7.1 supersuit-3.10.0 tinyscaler-1.2.8\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries for the environment and algorithms\n",
    "! pip install pettingzoo[mpe] supersuit stable-baselines3 pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5dbfe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3308228/1632111876.py:11: DeprecationWarning: The environment `pettingzoo.mpe` has been moved to `mpe2` and will be removed in a future release.Please update your imports.\n",
      "  from pettingzoo.mpe import simple_tag_v3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import multiprocessing\n",
    "\n",
    "# PettingZoo & SuperSuit\n",
    "import supersuit as ss\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "from pettingzoo.utils.wrappers import BaseParallelWrapper\n",
    "\n",
    "# Stable Baselines3\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Set up the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_cores = max(1, multiprocessing.cpu_count() - 2)\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab4894c",
   "metadata": {},
   "source": [
    "### Research Question 2\n",
    "**RQ2:** *How do the learning dynamics change when the reward structure shifts from pure competition to partial cooperation?*\n",
    "\n",
    "### ðŸ§ª Methodology\n",
    "To answer this, we introduce a **Reward Sharing Mechanism** controlled by a mixing parameter $\\alpha$ (alpha):\n",
    "* **$\\alpha = 1.0$ (Pure Competition):** Agents behave selfishly (Baseline).\n",
    "* **$\\alpha = 0.5$ (Partial Cooperation):** Agents share 50% of the reward with the team.\n",
    "* **$\\alpha = 0.0$ (Full Cooperation):** Agents share 100% of the reward (Team Spirit).\n",
    "\n",
    "We hypothesize that shifting $\\alpha$ from `1.0` to `0.5` will break the \"stalling\" equilibrium observed in earlier experiments and encourage coordinated encirclement strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c33316",
   "metadata": {},
   "source": [
    "Set up the enviroment and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d47cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardSharingWrapper(BaseParallelWrapper):\n",
    "    \"\"\"\n",
    "    A wrapper to modify the reward structure of the predators.\n",
    "    \n",
    "    Parameters:\n",
    "    - env: The PettingZoo parallel environment.\n",
    "    - alpha (float): The cooperation factor (0.0 to 1.0).\n",
    "        * alpha = 1.0: Pure Individual Reward (Competition).\n",
    "        * alpha = 0.0: Pure Shared Reward (Full Cooperation).\n",
    "        * alpha = 0.5: Mixed Strategy.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, alpha=1.0):\n",
    "        super().__init__(env)\n",
    "        self.alpha = alpha\n",
    "        # Identify which agents are predators (adversaries)\n",
    "        self.group_agents = [a for a in env.possible_agents if \"adversary\" in a]\n",
    "\n",
    "    def step(self, actions):\n",
    "        # Perform the standard environment step\n",
    "        obs, rewards, terminations, truncations, infos = self.env.step(actions)\n",
    "        \n",
    "        # Calculate the total reward achieved by the predator group\n",
    "        group_reward_sum = sum(rewards[a] for a in self.group_agents)\n",
    "        \n",
    "        # Redistribute rewards based on alpha\n",
    "        new_rewards = rewards.copy()\n",
    "        \n",
    "        for agent in self.group_agents:\n",
    "            individual_r = rewards[agent]\n",
    "            \n",
    "            # Formula: (alpha * Own_Reward) + ((1-alpha) * Group_Total)\n",
    "            # If alpha is 1, they only care about themselves.\n",
    "            # If alpha is 0, they only care about the team sum.\n",
    "            shared_r = (self.alpha * individual_r) + ((1 - self.alpha) * group_reward_sum)\n",
    "            new_rewards[agent] = shared_r\n",
    "            \n",
    "        return obs, new_rewards, terminations, truncations, infos\n",
    "\n",
    "def create_env(alpha=1.0):\n",
    "    \"\"\"\n",
    "    Creates a vectorized environment for faster training.\n",
    "    \n",
    "    Args:\n",
    "        alpha (float): The cooperation parameter.\n",
    "        num_envs (int): How many games to play in parallel (speed up).\n",
    "    \"\"\"\n",
    "    # 1. Initialize the base environment configuration\n",
    "    env = simple_tag_v3.parallel_env(\n",
    "        num_good=1, \n",
    "        num_adversaries=3, \n",
    "        num_obstacles=2, \n",
    "        max_cycles=50, \n",
    "        continuous_actions=False\n",
    "    )\n",
    "    \n",
    "    # 2. Apply the Reward Sharing Wrapper (RQ2 Implementation)\n",
    "    env = RewardSharingWrapper(env, alpha=alpha)\n",
    "    \n",
    "    # 3. Apply SuperSuit wrappers for SB3 compatibility\n",
    "    env = ss.pad_observations_v0(env) # Ensure equal observation size\n",
    "    env = ss.pad_action_space_v0(env) # Ensure equal action size\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env) # Convert to Vector Env\n",
    "\n",
    "    env = ss.concat_vec_envs_v1(\n",
    "        env, \n",
    "        num_vec_envs=1,  \n",
    "        num_cpus=num_cores,      \n",
    "        base_class=\"stable_baselines3\"\n",
    "    )\n",
    "    return env\n",
    "\n",
    "def run_experiment(alpha, total_timesteps=50000, rounds=3):\n",
    "    \"\"\"\n",
    "    Runs the full adversarial training experiment for a specific alpha.\n",
    "    \n",
    "    Args:\n",
    "        alpha: Cooperation level.\n",
    "        total_timesteps: Steps per training round (Higher = Better learning).\n",
    "        rounds: Number of alternating training cycles.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸš€ STARTING EXPERIMENT | Alpha: {alpha} \")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 1. Create separate environments for Predator and Prey models\n",
    "    # We use 8 parallel environments to speed up data collection (8x faster)\n",
    "    env_pred = create_env(alpha=alpha)\n",
    "    env_prey = create_env(alpha=alpha)\n",
    "    \n",
    "    # 2. Initialize PPO Models\n",
    "    # Note: 'device=device' ensures usage of GPU if available\n",
    "    predator_model = PPO(\"MlpPolicy\", env_pred, verbose=0, learning_rate=0.0003)\n",
    "    prey_model = PPO(\"MlpPolicy\", env_prey, verbose=0, learning_rate=0.0003)\n",
    "    \n",
    "    # 3. Adversarial Training Loop (Self-Play)\n",
    "    for i in range(rounds):\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nðŸ”„ Round {i+1}/{rounds} in progress...\")\n",
    "        \n",
    "        # Train Predators\n",
    "        predator_model.learn(total_timesteps=total_timesteps)\n",
    "        print(f\"   > Predators trained ({total_timesteps} steps)\")\n",
    "        \n",
    "        # Train Prey (Adapting to new predators)\n",
    "        prey_model.learn(total_timesteps=total_timesteps)\n",
    "        print(f\"   > Prey trained ({total_timesteps} steps)\")\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"   â±ï¸ Round finished in {elapsed:.2f} seconds.\")\n",
    "    \n",
    "    # Save Models\n",
    "    pred_path = f\"./models/predator_alpha_{alpha}\"\n",
    "    prey_path = f\"./models/prey_alpha_{alpha}\"\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    \n",
    "    predator_model.save(pred_path)\n",
    "    prey_model.save(prey_path)\n",
    "    \n",
    "    print(f\"\\nâœ… Experiment Complete. Models saved to 'models/' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be5b429",
   "metadata": {},
   "source": [
    "Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6967d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ STARTING EXPERIMENT | Alpha: 0.5 \n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Round 1/10 in progress...\n",
      "   > Predators trained (100000 steps)\n",
      "   > Prey trained (100000 steps)\n",
      "   â±ï¸ Round finished in 218.32 seconds.\n",
      "\n",
      "ðŸ”„ Round 2/10 in progress...\n",
      "   > Predators trained (100000 steps)\n",
      "   > Prey trained (100000 steps)\n",
      "   â±ï¸ Round finished in 218.37 seconds.\n",
      "\n",
      "ðŸ”„ Round 3/10 in progress...\n",
      "   > Predators trained (100000 steps)\n",
      "   > Prey trained (100000 steps)\n",
      "   â±ï¸ Round finished in 218.14 seconds.\n",
      "\n",
      "ðŸ”„ Round 4/10 in progress...\n",
      "   > Predators trained (100000 steps)\n",
      "   > Prey trained (100000 steps)\n",
      "   â±ï¸ Round finished in 218.82 seconds.\n",
      "\n",
      "ðŸ”„ Round 5/10 in progress...\n",
      "   > Predators trained (100000 steps)\n",
      "   > Prey trained (100000 steps)\n",
      "   â±ï¸ Round finished in 218.71 seconds.\n",
      "\n",
      "ðŸ”„ Round 6/10 in progress...\n",
      "   > Predators trained (100000 steps)\n",
      "   > Prey trained (100000 steps)\n",
      "   â±ï¸ Round finished in 268.38 seconds.\n",
      "\n",
      "ðŸ”„ Round 7/10 in progress...\n",
      "   > Predators trained (100000 steps)\n",
      "   > Prey trained (100000 steps)\n",
      "   â±ï¸ Round finished in 271.94 seconds.\n",
      "\n",
      "ðŸ”„ Round 8/10 in progress...\n",
      "   > Predators trained (100000 steps)\n",
      "   > Prey trained (100000 steps)\n",
      "   â±ï¸ Round finished in 272.27 seconds.\n",
      "\n",
      "ðŸ”„ Round 9/10 in progress...\n",
      "   > Predators trained (100000 steps)\n",
      "   > Prey trained (100000 steps)\n",
      "   â±ï¸ Round finished in 271.98 seconds.\n",
      "\n",
      "ðŸ”„ Round 10/10 in progress...\n",
      "   > Predators trained (100000 steps)\n",
      "   > Prey trained (100000 steps)\n",
      "   â±ï¸ Round finished in 272.39 seconds.\n",
      "\n",
      "âœ… Experiment Complete. Models saved to 'models/' folder.\n"
     ]
    }
   ],
   "source": [
    "# # Run baseline (Pure Competition)\n",
    "# run_experiment(alpha=1.0, total_timesteps=100000, rounds=10)\n",
    "\n",
    "# Run experimental (Partial Cooperation)\n",
    "# run_experiment(alpha=0.5, total_timesteps=100000, rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1b488",
   "metadata": {},
   "source": [
    "Evaluaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb40f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š EVALUATION RESULTS:\n",
      "\n",
      "ðŸš€ Starting Evaluation for Alpha 0.5...\n",
      "âœ… Evaluation complete for Alpha 0.5. Logs saved.\n",
      "\n",
      "ðŸš€ Starting Evaluation for Alpha 1.0...\n",
      "âœ… Evaluation complete for Alpha 1.0. Logs saved.\n",
      "Average Reward for Alpha = 0.5: 917.35\n",
      "Average Reward for Alpha = 1.0: 187.27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(alpha, episodes=20, max_cycles=50):\n",
    "    \"\"\"\n",
    "    Runs a test simulation and logs detailed metrics: \n",
    "    Captures, Collisions, Episode Sums, and Final Averages.\n",
    "    \"\"\"\n",
    "    model_path = f\"models/predator_alpha_{alpha}\"\n",
    "        \n",
    "    env = create_env(alpha=alpha)\n",
    "    pred_model = PPO.load(model_path)\n",
    "    \n",
    "    all_episode_rewards = []\n",
    "    total_captures = 0\n",
    "    total_collisions = 0\n",
    "    \n",
    "    log_file = f\"alpha_{alpha}_detailed_evaluation.txt\"\n",
    "    \n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(f\"DETAILED EVALUATION LOG | Alpha: {alpha}\\n\")\n",
    "        f.write(f\"Timestamp: {time.ctime()}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "    print(f\"\\nðŸš€ Starting Evaluation for Alpha {alpha}...\")\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs = env.reset()\n",
    "        ep_reward = 0\n",
    "        ep_captures = 0\n",
    "        ep_collisions = 0\n",
    "        \n",
    "        for step in range(max_cycles):\n",
    "            action, _ = pred_model.predict(obs)\n",
    "            obs, rewards, dones, infos = env.step(action)\n",
    "            \n",
    "            # 1. Sum rewards for the step [cite: 37]\n",
    "            predator_rewards = [rewards[a] for a in env.possible_agents if \"adversary\" in a]\n",
    "            prey_rewards = [rewards[a] for a in env.possible_agents if \"good\" in a]\n",
    "\n",
    "            step_reward = np.sum(predator_rewards)\n",
    "            ep_reward += step_reward\n",
    "            \n",
    "            # 2. Track Captures (Heuristic based on positive reward) [cite: 37]\n",
    "            if step_reward > 8: \n",
    "                ep_captures += 1\n",
    "            \n",
    "            # 3. Track Collisions (Fixed for list-based infos) [cite: 33, 38]\n",
    "            # If infos is a list (VectorEnv), we look at the first env's info\n",
    "            current_info = infos[0] if isinstance(infos, list) else infos\n",
    "            \n",
    "            # Safely iterate through agent info to count collisions\n",
    "            if isinstance(current_info, dict):\n",
    "                for agent_data in current_info.values():\n",
    "                    if isinstance(agent_data, dict) and agent_data.get(\"collision\", False):\n",
    "                        ep_collisions += 1\n",
    "\n",
    "        all_episode_rewards.append(ep_reward)\n",
    "        total_captures += ep_captures\n",
    "        total_collisions += ep_collisions\n",
    "        \n",
    "        # Log individual episode results\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"Episode {ep+1}: Total Reward: {ep_reward:.2f} | \"\n",
    "                    f\"Captures: {ep_captures} | Collisions: {ep_collisions}\\n\")\n",
    "\n",
    "    # Final Statistics for Alpha stability analysis [cite: 6, 43]\n",
    "    total_sum_all_episodes = np.sum(all_episode_rewards)\n",
    "    avg_reward = np.mean(all_episode_rewards)\n",
    "    avg_captures = total_captures / episodes\n",
    "    avg_collisions = total_collisions / episodes\n",
    "\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        f.write(f\"FINAL STATISTICS FOR ALPHA {alpha}:\\n\")\n",
    "        f.write(f\"Total Sum (All Episodes): {total_sum_all_episodes:.2f}\\n\")\n",
    "        f.write(f\"Average Reward: {avg_reward:.2f}\\n\")\n",
    "        f.write(f\"Total Captures: {total_captures} (Avg: {avg_captures:.2f}/ep)\\n\")\n",
    "        f.write(f\"Total Collisions: {total_collisions} (Avg: {avg_collisions:.2f}/ep)\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… Evaluation complete for Alpha {alpha}. Logs saved.\")\n",
    "    return avg_reward\n",
    "\n",
    "# Run Evaluation\n",
    "print(\"\\nðŸ“Š EVALUATION RESULTS:\")\n",
    "res = []\n",
    "for i in [0.5, 1.0]:\n",
    "    a = i #/ 10\n",
    "    score = evaluate_model(a)\n",
    "    res.append((i, score))\n",
    "\n",
    "[print(f\"Average Reward for Alpha = {a}: {score:.2f}\") for a, score in res]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
